---
title: "Evals & Testing"
description: "Ensure consistent agent quality with evaluations and testing"
icon: "check-circle"
---

## Overview

Evaluations (evals) help ensure your agents perform consistently and meet quality standards. Test agent capabilities, validate outputs, and track performance over time.

## Why Evals Matter

<CardGroup cols={2}>
  <Card title="Quality Assurance" icon="shield-check">
    Ensure agents produce consistent, high-quality outputs
  </Card>

  <Card title="Regression Detection" icon="bug">
    Catch issues before they reach production
  </Card>

  <Card title="Performance Tracking" icon="chart-line">
    Monitor improvements and degradations over time
  </Card>

  <Card title="Confidence" icon="thumbs-up">
    Deploy with confidence knowing agents work as expected
  </Card>
</CardGroup>

## Types of Evals

### 1. Unit Tests

Test specific agent capabilities in isolation:

```typescript
import { describe, it, expect } from 'vitest';

describe('Agent File Operations', () => {
  it('should read CSV files', async () => {
    const result = await agentbase.runAgent({
      message: "Read data.csv and return the first row",
      mode: "base"
    });

    expect(result.content).toContain('header1,header2');
    expect(result.success).toBe(true);
  });

  it('should handle missing files gracefully', async () => {
    const result = await agentbase.runAgent({
      message: "Read nonexistent.csv",
      mode: "base"
    });

    expect(result.content).toMatch(/not found|does not exist/i);
  });

  it('should create files with correct content', async () => {
    const content = 'test data';

    const result = await agentbase.runAgent({
      message: `Create a file named test.txt with content: ${content}`,
      mode: "base"
    });

    expect(result.success).toBe(true);
    // Verify file was created in next request
  });
});
```

### 2. Integration Tests

Test end-to-end workflows and multi-step processes:

```typescript
describe('Agent Workflows', () => {
  it('should complete data processing workflow', async () => {
    // Step 1: Upload and process data
    const upload = await agentbase.runAgent({
      message: "Create data.json with sample e-commerce data",
      mode: "base"
    });

    expect(upload.success).toBe(true);

    // Step 2: Analyze data
    const analyze = await agentbase.runAgent({
      message: "Analyze data.json and calculate total revenue",
      session: upload.session,
      mode: "base"
    });

    expect(analyze.content).toMatch(/total.*revenue/i);
    expect(analyze.content).toMatch(/\$[\d,]+/);

    // Step 3: Generate report
    const report = await agentbase.runAgent({
      message: "Create a summary report",
      session: analyze.session,
      mode: "base"
    });

    expect(report.content).toContain('Summary');
    expect(report.content).toContain('Revenue');
  });
});
```

### 3. Performance Benchmarks

Track performance metrics over time:

```typescript
describe('Agent Performance', () => {
  const benchmarks = {
    'simple-query': { maxSteps: 2, maxTime: 3000 },
    'file-processing': { maxSteps: 5, maxTime: 10000 },
    'web-scraping': { maxSteps: 8, maxTime: 15000 }
  };

  it('should complete simple queries efficiently', async () => {
    const startTime = Date.now();
    let steps = 0;

    const result = await agentbase.runAgent({
      message: "What is 2+2?",
      mode: "flash",
      stream: true
    });

    for await (const event of result) {
      if (event.type === 'agent_step') steps++;
    }

    const duration = Date.now() - startTime;

    expect(steps).toBeLessThanOrEqual(benchmarks['simple-query'].maxSteps);
    expect(duration).toBeLessThanOrEqual(benchmarks['simple-query'].maxTime);
  });

  it('should process files within performance limits', async () => {
    const startTime = Date.now();
    let steps = 0;

    const result = await agentbase.runAgent({
      message: "Read sample.csv and count rows",
      mode: "base",
      stream: true
    });

    for await (const event of result) {
      if (event.type === 'agent_step') steps++;
    }

    const duration = Date.now() - startTime;

    expect(steps).toBeLessThanOrEqual(benchmarks['file-processing'].maxSteps);
    expect(duration).toBeLessThanOrEqual(benchmarks['file-processing'].maxTime);
  });
});
```

### 4. Output Quality Evals

Evaluate the quality and correctness of agent outputs:

```typescript
describe('Agent Output Quality', () => {
  it('should generate valid JSON', async () => {
    const result = await agentbase.runAgent({
      message: "Create a JSON object with user data: name=John, age=30",
      mode: "base"
    });

    // Extract JSON from response
    const jsonMatch = result.content.match(/\{[\s\S]*\}/);
    expect(jsonMatch).toBeTruthy();

    const parsed = JSON.parse(jsonMatch[0]);
    expect(parsed.name).toBe('John');
    expect(parsed.age).toBe(30);
  });

  it('should provide accurate calculations', async () => {
    const result = await agentbase.runAgent({
      message: "Calculate the sum of numbers from 1 to 100",
      mode: "base"
    });

    expect(result.content).toContain('5050');
  });

  it('should follow formatting instructions', async () => {
    const result = await agentbase.runAgent({
      message: "List 3 fruits in markdown format",
      mode: "base"
    });

    // Check for markdown list format
    expect(result.content).toMatch(/[-*]\s+\w+/);
  });
});
```

### 5. LLM-as-Judge Evals

Use another LLM to evaluate agent outputs:

```typescript
async function evaluateWithLLM(
  task: string,
  agentOutput: string,
  criteria: string[]
) {
  const evaluation = await agentbase.runAgent({
    message: `Evaluate this agent response on a scale of 1-10 for each criterion.

Task: ${task}
Response: ${agentOutput}

Criteria to evaluate:
${criteria.map((c, i) => `${i + 1}. ${c}`).join('\n')}

Provide scores and brief explanations in JSON format:
{
  "scores": { "criterion1": score, ... },
  "overallScore": average,
  "explanation": "brief explanation"
}`,
    mode: "base"
  });

  return JSON.parse(evaluation.content);
}

describe('Agent Quality (LLM-as-Judge)', () => {
  it('should generate helpful code explanations', async () => {
    const result = await agentbase.runAgent({
      message: "Explain how a binary search algorithm works",
      mode: "base"
    });

    const evaluation = await evaluateWithLLM(
      "Explain binary search",
      result.content,
      [
        "Accuracy: Is the explanation technically correct?",
        "Clarity: Is it easy to understand?",
        "Completeness: Does it cover key concepts?",
        "Examples: Does it include helpful examples?"
      ]
    );

    expect(evaluation.overallScore).toBeGreaterThan(7);
  });
});
```

## Building an Eval Framework

### Structured Eval Runner

```typescript
interface EvalCase {
  name: string;
  message: string;
  mode: string;
  validate: (result: any) => Promise<EvalResult>;
}

interface EvalResult {
  passed: boolean;
  score?: number;
  message?: string;
  metadata?: any;
}

class EvalRunner {
  private results: Map<string, EvalResult> = new Map();

  async runEval(evalCase: EvalCase): Promise<EvalResult> {
    console.log(`Running eval: ${evalCase.name}`);

    try {
      const result = await agentbase.runAgent({
        message: evalCase.message,
        mode: evalCase.mode
      });

      const evalResult = await evalCase.validate(result);
      this.results.set(evalCase.name, evalResult);

      console.log(`✅ ${evalCase.name}: ${evalResult.passed ? 'PASS' : 'FAIL'}`);
      return evalResult;

    } catch (error) {
      const failResult = {
        passed: false,
        message: `Error: ${error.message}`
      };

      this.results.set(evalCase.name, failResult);
      console.log(`❌ ${evalCase.name}: FAIL`);

      return failResult;
    }
  }

  async runAll(evals: EvalCase[]): Promise<void> {
    for (const evalCase of evals) {
      await this.runEval(evalCase);
    }

    this.printSummary();
  }

  printSummary() {
    const total = this.results.size;
    const passed = Array.from(this.results.values())
      .filter(r => r.passed).length;

    console.log(`\n=== Eval Summary ===`);
    console.log(`Total: ${total}`);
    console.log(`Passed: ${passed}`);
    console.log(`Failed: ${total - passed}`);
    console.log(`Pass Rate: ${((passed / total) * 100).toFixed(1)}%\n`);
  }

  getResults() {
    return this.results;
  }
}

// Usage
const runner = new EvalRunner();

const evals: EvalCase[] = [
  {
    name: 'File Read',
    message: 'Read data.csv and return first row',
    mode: 'base',
    validate: async (result) => ({
      passed: result.content.includes('header'),
      message: 'Should contain header row'
    })
  },
  {
    name: 'Math Calculation',
    message: 'Calculate 15 * 23',
    mode: 'flash',
    validate: async (result) => ({
      passed: result.content.includes('345'),
      message: 'Should calculate correct result'
    })
  }
];

await runner.runAll(evals);
```

### Continuous Eval Pipeline

```typescript
// Run evals on schedule or before deployment
async function continuousEvals() {
  const runner = new EvalRunner();

  // Load eval cases
  const evals = await loadEvalCases('./evals');

  // Run all evals
  await runner.runAll(evals);

  // Get results
  const results = runner.getResults();
  const passRate = calculatePassRate(results);

  // Store results
  await storeResults({
    timestamp: new Date(),
    results,
    passRate
  });

  // Alert if pass rate drops
  if (passRate < 0.95) {
    await alertTeam({
      message: `Eval pass rate dropped to ${passRate}%`,
      results
    });
  }

  return results;
}

// Run daily
setInterval(continuousEvals, 24 * 60 * 60 * 1000);
```

## Eval Best Practices

<AccordionGroup>
  <Accordion title="Test Real Use Cases" icon="users">
    Base evals on actual user tasks and common scenarios:

    ```typescript
    const realWorldEvals = [
      {
        name: 'Customer Support - Order Status',
        message: 'Check order status for #12345',
        validate: /* ... */
      },
      {
        name: 'Data Analysis - Sales Report',
        message: 'Generate weekly sales report',
        validate: /* ... */
      }
    ];
    ```
  </Accordion>

  <Accordion title="Test Edge Cases" icon="triangle-exclamation">
    Include edge cases and error conditions:

    ```typescript
    const edgeCaseEvals = [
      {
        name: 'Empty File',
        message: 'Process empty.csv',
        validate: (r) => ({
          passed: r.content.includes('empty') || r.content.includes('no data')
        })
      },
      {
        name: 'Invalid URL',
        message: 'Fetch data from http://invalid-url',
        validate: (r) => ({
          passed: r.content.includes('error') || r.content.includes('failed')
        })
      }
    ];
    ```
  </Accordion>

  <Accordion title="Track Over Time" icon="chart-line">
    Store eval results to track trends:

    ```typescript
    interface EvalHistory {
      timestamp: Date;
      results: Map<string, EvalResult>;
      passRate: number;
      metadata: {
        version: string;
        mode: string;
      };
    }

    const history: EvalHistory[] = [];

    async function trackEvals() {
      const results = await runEvals();

      history.push({
        timestamp: new Date(),
        results,
        passRate: calculatePassRate(results),
        metadata: {
          version: '1.0',
          mode: 'base'
        }
      });

      // Detect regressions
      if (history.length > 1) {
        const current = history[history.length - 1];
        const previous = history[history.length - 2];

        if (current.passRate < previous.passRate - 0.05) {
          console.warn('Performance regression detected!');
        }
      }
    }
    ```
  </Accordion>

  <Accordion title="Automate in CI/CD" icon="gear">
    Run evals automatically before deployment:

    ```yaml
    # .github/workflows/evals.yml
    name: Agent Evals

    on: [push, pull_request]

    jobs:
      evals:
        runs-on: ubuntu-latest
        steps:
          - uses: actions/checkout@v2
          - uses: actions/setup-node@v2
          - run: npm install
          - run: npm run evals
          - name: Check pass rate
            run: |
              if [ $PASS_RATE -lt 95 ]; then
                echo "Eval pass rate too low: $PASS_RATE%"
                exit 1
              fi
    ```
  </Accordion>
</AccordionGroup>

## Eval Metrics

Track these key metrics:

| Metric | Description | Target |
|--------|-------------|--------|
| **Pass Rate** | Percentage of evals that pass | > 95% |
| **Avg Score** | Average quality score across evals | > 8/10 |
| **Performance** | Avg time and steps per eval | Within benchmarks |
| **Reliability** | Success rate (no errors) | > 98% |

## Next Steps

<CardGroup cols={2}>
  <Card title="Traces" icon="bug" href="/improve/traces">
    Debug failed evals with traces
  </Card>

  <Card title="Performance" icon="gauge" href="/improve/performance">
    Optimize slow eval cases
  </Card>

  <Card title="Cost Tracking" icon="dollar" href="/improve/cost-tracking">
    Monitor eval costs
  </Card>

  <Card title="Testing Guide" icon="vial" href="/build/use-cases">
    More testing patterns and examples
  </Card>
</CardGroup>
